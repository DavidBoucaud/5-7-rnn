{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\David\\Documents\\code\\Mod_5\\5-7-rnn\\dataset\\aclImdb'\n",
    "\n",
    "def read_imdb(data_dir, is_train):\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',\n",
    "                                   label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data, train_labels = read_imdb(data_dir, is_train=True)\n",
    "test_data, test_labels = read_imdb(data_dir, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-transparency",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis\n",
    "\n",
    "Using the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), we want to do a regression model that predict the ratings are on a 1-10 scale. You have an example train and test set in the `dataset` folder.\n",
    "\n",
    "### 1.1 Regression Model\n",
    "\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset\n",
    "\n",
    "### 1.2 RNN model\n",
    "\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get both datasets numerized and padded right from the beggining.\n",
    "tokenize = Tokenizer(lower = True)\n",
    "tokenize.fit_on_texts(train_data)\n",
    "X_coded = tokenize.texts_to_sequences(train_data)\n",
    "X_train = pad_sequences(X_coded, padding='post')\n",
    "y_train = train_labels\n",
    "\n",
    "tokenize_ = Tokenizer(lower = True)\n",
    "tokenize_.fit_on_texts(test_data)\n",
    "X_coded = tokenize_.texts_to_sequences(test_data)\n",
    "X_test = pad_sequences(X_coded, padding='post')\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_null, y_train, Y_null = train_test_split(X_train, y_train, test_size=0.2) #Just an easy way to kill some data\n",
    "\n",
    "X_null, X_test, Y_null, y_test = train_test_split(X_train, y_train, test_size=0.2) #Just an easy way to kill some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 38s 156ms/step - loss: 0.6991 - accuracy: 0.5076 - val_loss: 0.7027 - val_accuracy: 0.4948\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 35s 148ms/step - loss: 0.6917 - accuracy: 0.5209 - val_loss: 0.6913 - val_accuracy: 0.5146\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 36s 151ms/step - loss: 0.6855 - accuracy: 0.5666 - val_loss: 0.6904 - val_accuracy: 0.5444\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 37s 158ms/step - loss: 0.6818 - accuracy: 0.6142 - val_loss: 0.6910 - val_accuracy: 0.5056\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 48s 203ms/step - loss: 0.6786 - accuracy: 0.6384 - val_loss: 0.6896 - val_accuracy: 0.5324\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 44s 189ms/step - loss: 0.6741 - accuracy: 0.6868 - val_loss: 0.6880 - val_accuracy: 0.5666\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 38s 162ms/step - loss: 0.6702 - accuracy: 0.6877 - val_loss: 0.6888 - val_accuracy: 0.5132\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 39s 164ms/step - loss: 0.6662 - accuracy: 0.6999 - val_loss: 0.6858 - val_accuracy: 0.5812\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 39s 165ms/step - loss: 0.6617 - accuracy: 0.7393 - val_loss: 0.6848 - val_accuracy: 0.5694\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 40s 172ms/step - loss: 0.6565 - accuracy: 0.7688 - val_loss: 0.6829 - val_accuracy: 0.5952\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab42f9d30>"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "#1.1\n",
    "vocabulary_size = len(tokenize.word_counts.keys())+1\n",
    "max_words = len(max((X_train), key=len))\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.736"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "#Now for the test\n",
    "pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "y = np.argmax(y_test, axis=-1)\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "313/313 [==============================] - 2116s 7s/step - loss: 0.6934 - accuracy: 0.4996 - val_loss: 0.6937 - val_accuracy: 0.4916\n",
      "Epoch 2/2\n",
      "313/313 [==============================] - 1996s 6s/step - loss: 0.6935 - accuracy: 0.4944 - val_loss: 0.6937 - val_accuracy: 0.4916\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab5344370>"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "#1.2\n",
    "vocabulary_size = len(tokenize.word_counts.keys())+1\n",
    "max_words = len(max((X_train), key=len))\n",
    "embedding_size = 100\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.50032"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "#Now for the test\n",
    "pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "y = np.argmax(y_test, axis=-1)\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-chosen",
   "metadata": {},
   "source": [
    "# 2. (evil) XOR Problem\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "\n",
    "### 2.1 \n",
    "\n",
    "Generate a dataset of random <=100,000 binary strings of equal length <= 50. Train the LSTM; what is the maximum length you can train up to with precisison?\n",
    "    \n",
    "\n",
    "### 2.2\n",
    "\n",
    "Generate a dataset of random <=200,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "authentic-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1\n",
    "\n",
    "#I'll be honest, I tried everything right up to 50 columns, my bad NN still hits the same accuracy.\n",
    "XOs = np.zeros((100000, 30), dtype=float)\n",
    "labels = np.zeros(100000)\n",
    "\n",
    "for row in range(100000):\n",
    "    XOs[row] = np.random.randint(0, 2, 30)\n",
    "    labels[row] = XOs[row].sum() % 2 # simple math trick that gives the correct label.\n",
    "\n",
    "X = XOs\n",
    "y = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 205s 80ms/step - loss: 0.6934 - accuracy: 0.5061 - val_loss: 0.6932 - val_accuracy: 0.4992\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 190s 76ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5008\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.6932 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5008\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 220s 88ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6933 - val_accuracy: 0.4992\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 209s 84ms/step - loss: 0.6932 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.4992\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 209s 84ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6932 - val_accuracy: 0.4992\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 202s 81ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.4992\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 201s 81ms/step - loss: 0.6932 - accuracy: 0.5021 - val_loss: 0.6932 - val_accuracy: 0.5008\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 200s 80ms/step - loss: 0.6932 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.4992\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 197s 79ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6933 - val_accuracy: 0.4992\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ae7ebae80>"
      ]
     },
     "metadata": {},
     "execution_count": 246
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X.reshape((len(X), 30, 1)), y, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.2\n",
    "#This is the setup I started with. Its works at generating random length rows, but the keras model just wouldn't take samples of varying sizes.\n",
    "\n",
    "XOs = []\n",
    "\n",
    "for row in range(200000):\n",
    "    num = np.random.randint(0, 50)\n",
    "    temp = np.zeros_like(range(num))\n",
    "    XOs.append(np.zeros_like(range(num)))\n",
    "\n",
    "labels = np.zeros(200000)\n",
    "\n",
    "for row in range(200000):\n",
    "    XOs[row] = np.random.randint(0, 2, len(XOs[row]))\n",
    "    labels[row] = XOs[row].sum() % 2 # simple math trick that gives the correct label.\n",
    "\n",
    "X = XOs\n",
    "y = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This (technically) fits the question, but it is not random lengths. The integers are assigned at random length, meaning that the array is zero-padded with random lengths to the paddings.\n",
    "\n",
    "XOs = np.zeros((100000, 50), dtype=float)\n",
    "labels = np.zeros(100000)\n",
    "\n",
    "for row in range(100000):\n",
    "    num = np.random.randint(0, 50)\n",
    "    for i in range(np.random.randint(0, 50)):\n",
    "        XOs[row][i] = np.random.randint(0, 2)\n",
    "    labels[row] = XOs[row].sum() % 2 # simple math trick that gives the correct label.\n",
    "\n",
    "X = XOs\n",
    "y = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = XOs\n",
    "y = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 340s 132ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.4845\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 336s 134ms/step - loss: 0.6931 - accuracy: 0.5066 - val_loss: 0.6927 - val_accuracy: 0.5154\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 328s 131ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6679 - val_accuracy: 0.5252\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 327s 131ms/step - loss: 0.6721 - accuracy: 0.5281 - val_loss: 0.6933 - val_accuracy: 0.4845\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 296s 118ms/step - loss: 0.6933 - accuracy: 0.5023 - val_loss: 0.6927 - val_accuracy: 0.5155\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14b18288850>"
      ]
     },
     "metadata": {},
     "execution_count": 380
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X.reshape((len(X), 50, 1)), y, epochs=5, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd02b98e5309e6e3b0bade2ed4a1aa225e8ff7275f11bed2b4c0572310c8cf94ab4",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}